pml_training <- read_csv("Data/pml-training.csv")
install.packages(tidyverse)
install.packages("tidyverse")
library(dplyr)
library(tidyverse)
pml_training <- read_csv("Data/pml-training.csv")
pml_testing <- read_csv("Data/pml-testing.csv")
dim(pml_training)
dim(pml_testing)
summarise(pml_training)
View(pml_training)
table(pml_training$classe)
pml_training <- as.factor(pml_training)
pml_training$classe <- as.factor(pml_training$classe)
pml_training <- read_csv("Data/pml-training.csv")
pml_testing <- read_csv("Data/pml-testing.csv")
rm(list=ls())
pml_training <- read_csv("Data/pml-training.csv")
pml_testing <- read_csv("Data/pml-testing.csv")
pml_training$classe <- as.factor(pml_training$classe)
pml_testing$classe <- as.factor(pml_testing$classe)
getwd()
knitr::opts_chunk$set(echo = TRUE)
summary(pml_training)
set.seed(1903)
rm(list=ls())
training_set <- read.csv("Data/pml-training.csv")
validation_set <- read.csv("Data/pml-testing.csv")
set.seed(1234)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)  # Change this to .75
library(rpart)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)  # Change this to .75
library(rpart.plot)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)  # Change this to .75
library(caret)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)  # Change this to .75
trainData<- training_set[, colSums(is.na(training_set)) == 0]
validData <- validation_set[, colSums(is.na(validation_set)) == 0]
head(training_set)
View(training_set)
trainData2 <- trainData %>% select(-c("X1","user_name","raw_timestamp_part1",
"raw_timestamp_part2", "cvtd_timestamp",
"new_window", "new_number"))
trainData2 <- trainData %>% select(-c("X","user_name","raw_timestamp_part1",
"raw_timestamp_part2", "cvtd_timestamp",
"new_window", "new_number"))
trainData2 <- trainData %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "new_number"))
trainData2 <- trainData %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window"))
validData2 <- validData2 %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window"))
validData2 <- validData %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window"))
set.seed(1234)
inTrain <- createDataPartition(trainData2$classe, p = 0.7, list = FALSE)  # Change this to .75
trainData <- trainData[inTrain, ]
testData <- trainData[-inTrain, ]
trainData2 <- trainData[inTrain, ]
testData2 <- trainData[-inTrain, ]
rm(list=ls())
training_set <- read.csv("Data/pml-training.csv")
validation_set <- read.csv("Data/pml-testing.csv")
trainData<- training_set[, colSums(is.na(training_set)) == 0]
validData <- validation_set[, colSums(is.na(validation_set)) == 0]
rm(training_set, validation_set)
trainData <- trainData %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window"))
validData <- validData %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window"))
set.seed(1234)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)  # Change this to .75
trainData <- trainData[inTrain, ]
testData <- trainData[-inTrain, ]
NZV <- nearZeroVar(trainData)
trainData <- trainData[, -NZV]
testData  <- testData[, -NZV]
dim(trainData)
cor_mat <- cor(trainData[, -53])
corrplot(cor_mat, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
install.packages("corrplot")
library(corrplot)
corrplot(cor_mat, order = "FPC", method = "color", type = "upper",
tl.cex = 0.8, tl.col = rgb(0, 0, 0))
highlyCorrelated = findCorrelation(cor_mat, cutoff=0.75)
names(trainData)[highlyCorrelated]
decisionTreeMod1 <- rpart(classe ~ ., data=trainData, method="class")
fancyRpartPlot(decisionTreeMod1)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
fancyRpartPlot(decisionTreeMod1)
library(caret)
library(kernlab)
library(dplyr)
library(tidyverse)
training_set <- read.csv("Data/pml-training.csv")
validation_set <- read.csv("Data/pml-testing.csv") # We leave this to the end
training_set$classe <- as.factor(training_set$classe)
trainData<- training_set[, colSums(is.na(training_set)) == 0]
trainData <- trainData %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window"))
validData <- validation_set[, colSums(is.na(validation_set)) == 0]
validData <- validData %>% select(-c("X","user_name","raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window"))
mostlyNA <- sapply(trainData, function(x) mean(is.na(x))) > 0.95
trainData <- trainData[, mostlyNA==F]
NZV <- nearZeroVar(trainData)
trainData <- trainData[, -NZV]
testData  <- testData[, -NZV]
dim(trainData)
inTrain <- createDataPartition(trainData$classe,
p = 0.7, list = FALSE)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
?train
modelFit_1 <- train(classe ~ ., data = training, method = "class",)
library(rpart)
library(rpart.plot)
modelFit_1 <- train(classe ~ ., data = training, method = "class",)
library(randomForest)
modelFit_1 <- train(classe ~ ., data = training, method = "class",)
modelFit_1 <- rpart(classe ~ ., data = training, method = "class",)
modelFit_1
fancyRpartPlot(modelFit_1)
library(rpart.plot)
fancyRpartPlot(modelFit_1)
library(RColorBrewer)
modelFit_1 <- rpart(classe ~ ., data = training, method = "class",)
modelFit_1 # Look at model
fancyRpartPlot(modelFit_1)
install.packages("rpart.plot")
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modelFit_1)
library(rpart.plot)
library(rattle)
fancyRpartPlot(modelFit_1)
fancyRpartPlot(modelFit_1)
predictTreeMod1 <- predict(modelFit_1, testing, type = "class")
cmtree <- confusionMatrix(predictTreeMod1, testData$classe)
library(gbm)
cmtree <- confusionMatrix(predictTreeMod1, testData$classe)
library(caret)
cmtree <- confusionMatrix(predictTreeMod1, testData$classe)
cmtree <- confusionMatrix(predictTreeMod1, testing$classe)
cmtree
cmtree
modelFit_2<- randomForest(classe ~. , data=training)
library(caret)
library(kernlab)
library(dplyr)
library(tidyverse)
library(rpart)
library(randomForest)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(gbm)
modelFit_2<- randomForest(classe ~. , data=training)
Pred_modelFit_2 <- predict(modelFit_2, testing, type = "class")
fancyRpartPlot(modelFit_1)
confusionMatrix(Pred_modelFit_2, myTesting$classe)
confusionMatrix(Pred_modelFit_2, testing$classe)
plot(modelFit_1)
plot(modelFit_2)
qplot(modelFit_2)
plot(modelFit_2)
title(main = "Random Forest Errors")
plot(modelFit_2, main = "Random Forest Errors")
plot(modelFit_2, main = "Random Forest Errors",
sub = "Number of Trees")
plot(modelFit_2, main = "Random Forest Errors",
xlab = "Number of Trees")
plot(modelFit_2, main = "Random Forest Errors",
xlab = "Number of Trees")
plot(modelFit_2, main = "Random Forest Errors",
xlab = "Number of Trees",
sub = NULL)
fancyRpartPlot(modelFit_1)
plot(modelFit_2, main = "Random Forest Errors",
xlab = "Number of Trees",
sub = NULL)
plot(modelFit_2, main = "Random Forest Errors",
xlab = "Number of Trees",
ylab = "Error Rate")
plot(modelFit_2)
plot(modelFit_2, main = "Random Forest Errors",
xlab = "Number of Trees",
ylab = "Error Rate")
plot(modelFit_2, main = "Random Forest Errors",
#xlab = "Number of Trees",
ylab = "Error Rate")
plot(modelFit_2, main = "Random Forest Errors",
#xlab = "Number of Trees",
ylab = "Error Rate")
plot(modelFit_2, main = "Random Forest Errors")
modelFit_3 <- train(classe ~., method = "gbm", data = training, verbose = FALSE)
print(modelFit_3)
qplot(predict(modelFit_3, testing), wage, data=testing)
qplot(predict(modelFit_3, testing), classe, data=testing)
qplot(predict(modelFit_3, testing), classe, data=validData)
qplot(predict(modelFit_3, testing), classe, data = training)
qplot(predict(modelFit_3, testing), classe, data = testing)
library(caret)
rm(list=ls())
library(caret)
library(kernlab)
library(dplyr)
library(tidyverse)
library(rpart)
library(randomForest)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(gbm)
class(training)
training <- as.matrix(training)
class(training)
modelFit_1 <- rpart(classe ~ ., data = training, method = "class")
pacman::p_load(caret, kernlab, dplyr, tidyverse, rpart,
randomForest, rpart.plot, rattle, RColorBrewer, gbm)
library(rpart)
modelFit_1 <- rpart(classe ~ ., data = training, method = "class")
training <- as.data.frame(training)
library(caret)
library(kernlab)
library(dplyr)
library(tidyverse)
library(rpart)
library(randomForest)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(gbm)
library(ranger)
training_set <- read.csv("Data/pml-training.csv")
validation_set <- read.csv("Data/pml-testing.csv") # We leave this to the end
training_set$classe <- as.factor(training_set$classe)
KeepThrow <- colSums(is.na(training_set) |training_set=="")
Observ <- which(KeepThrow == 0)
training_set <- training_set[, Observ]
training_set <- training_set[, -c(1:7)]
inTrain <- createDataPartition(training_set$classe,
p = 0.7, list = FALSE)
training <- training_set[inTrain, ]
testing <- training_set[-inTrain, ]
modelFit_2<- ranger(classe ~. , data=training,
# mtry = 30,
respect.unordered.factors = "order")
modFit_2_2 <- randomForest(classe~., data = training)
print(modFit)
print(modelFit_2)
print(modFit_2_2)
predict1 <- predict(modelFit_2, testing, type = "class")
predict1 <- predict(modelFit_2, testing)
confusionMatrix(testing$classe, predict1)
predict1 <- predict(modelFit_2, testing)
confusionMatrix(testing$classe, predict1)
confusionMatrix(predict1, testing$classe, )
modelFit_2 <- randomForest(classe~., data = training)
modelFit_2
plot(modelFit_2)
plot(modelFit_2, main = "RF Model")
plot(modelFit_2, main = "RF Model",
ylab = "Error Rate",
xlab = "N Trees")
plot(modelFit_2, main = "RF Model",
ylab = "Error Rate")
plot(modelFit_2, main = "RF Model")
oob.err <- double(52)
test.err <- double(52)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:13)
{
rf <- randomForest(classe ~ . , data = training , mtry = mtry,ntree= 400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred <- predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry] = with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
for(mtry in 1:13)
{
rf <- randomForest(classe ~ . , data = training , mtry = mtry,ntree= 400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred <- predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry] = with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
for(mtry in 1:52)
{
rf <- randomForest(classe ~ . , data = training , mtry = mtry, ntree= 400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred <- predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry] = with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
rf <- randomForest(classe ~ . , data = training , mtry = mtry, ntree= 400)
oob.err[mtry] = rf$mse[400]
oob.err[mtry] <- rf$mse[400] #Error of all Trees fitted
oob.err
oob.err <- double(52)
test.err <- double(52)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:52)
{
rf <- randomForest(classe ~ . , data = training , mtry = mtry, ntree= 400)
oob.err[mtry] <- rf$mse[400] #Error of all Trees fitted
pred <- predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry] = with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
oob.err=double(13)
test.err=double(13)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:13)
{
rf=randomForest(classe ~ . , data = training_set , subset = training,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry]= with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
oob.err=double(52)
test.err=double(52)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:52)
{
rf=randomForest(classe ~ . , data = training_set , subset = training,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry]= with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
oob.err=double(52)
test.err=double(52)
for(mtry in 1:52)
{
rf=randomForest(classe ~ . , data = training_set , subset = training,mtry=mtry,ntree=400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred<-predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry]= with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
oob.err <- double(7)
test.err <- double(7)
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:7)
{
rf <- randomForest(classe ~ . , data = training , mtry = mtry, ntree= 400)
oob.err[mtry] <- rf$mse[400] #Error of all Trees fitted
pred <- predict(rf,testing) #Predictions on Test Set for each Tree
test.err[mtry] = with(testing, mean( (classe - pred)^2)) #Mean Squared Test Error
cat(mtry," ") #printing the output to the console
}
rm(list=;s())
rm(list=ls())
# Random example on test set
library(caret)
library(kernlab)
library(dplyr)
library(tidyverse)
library(rpart)
library(randomForest)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(gbm)
library(ranger)
training_set <- read.csv("Data/pml-training.csv")
validation_set <- read.csv("Data/pml-testing.csv") # We leave this to the end
# CLEAN THE DATA
# Before we start we need to convert the variable "classe" to a factor
training_set$classe <- as.factor(training_set$classe)
# Remove rows that do not have any data
# Take the training_set and sum the records where the data is either NA or empty.
KeepThrow <- colSums(is.na(training_set) |training_set=="")
# Keep the records where we have fields that have observations
Observ <- which(KeepThrow == 0)
training_set <- training_set[, Observ]
# Remove first 7 observations as they do not help us in predicting the outcome of our study.
training_set <- training_set[, -c(1:7)]
inTrain <- createDataPartition(training_set$classe,
p = 0.7, list = FALSE)
training <- training_set[inTrain, ]
testing <- training_set[-inTrain, ]
# TrainControl:
trControl <- trainControl(method = "cv",
number = 10,
search = "grid")
rf_default <- train(classe ~.,
data = training,
method = "rf",
metric = "Accuracy",
trControl = trControl)
modelFit_2 <- randomForest(classe~., data = training)
print(modelFit_2)
?randomForest
rf_default <- train(classe ~.,
data = training,
method = "rf",
metric = "Accuracy",
trControl = trControl)
print(rf_default)
set.seed(1234)
tuneGrid <- expand.grid(.mtry = c(1: 10))
rf_mtry <- train(classe ~.,
data = training,
method = "rf",
metric = "Accuracy",
tuneGrid = tuneGrid,
trControl = trControl,
importance = TRUE,
nodesize = 14,
ntree = 300)
rf_mtry <- train(classe ~.,
data = training,
method = "rf",
metric = "Accuracy",
tuneGrid = tuneGrid,
trControl = trControl,
importance = TRUE,
nodesize = 14,
ntree = 300)
print(rf_default)
predict(rf_default, newdata = testing)
pred_rf <- predict(rf_default, newdata = testing)
confmatrix <- confusionMatrix(pred_rf, testing$classe)
confmatrix
varImpPlot(rf_default)
version
modelFit_3 <- train(classe ~., method = "gbm", data = training, verbose = FALSE)
?trControl
??trControl
?trainControl
?rpart
?trainControl
plot(modelFit_2, main = "RF Model")
confusionMatrix(predict1, testing$classe, )
predict1 <- predict(modelFit_2, testing)
confusionMatrix(predict1, testing$classe, )
default_rmse <- sqrt(modelFit_2$prediction.error)
validation_set <- validation_set[, Observ]
validation_set <- validation_set[, -c(1:7)]
predict_FINAL <- predict(modelFit_2, validation_set, type = "class")
print(predict_FINAL)
postResample(modelFit_1, actual)
postResample(modelFit_2, actual)
plot(modelFit_2, log = "y", lwd = 2, main = "Random forest accuracy", xlab = "Predictors",
ylab = "Accuracy")
plot(modelFit_2, log = "y", lwd = 2, main = "Random forest accuracy")
plot(modelFit_2, log = "y", lwd = 2, main = "Random forest accuracy")
plot(modelFit_2$err.rate, log = "y", lwd = 2, main = "Random forest accuracy")
plot(modelFit_2$predicted, log = "y", lwd = 2, main = "Random forest accuracy")
modelFit_2 <- randomForest(classe~., data = training)
print(modelFit_2)
plot(modelFit_2, main = "RF Model")
predict1 <- predict(modelFit_2, testing)
confusionMatrix(predict1, testing$classe)
predict_FINAL <- predict(modelFit_2, validation_set, type = "class")
print(predict_FINAL)
pred_model2 <- train(classe ~.,
data = training,
method = "rf",
metric = "Accuracy",
trControl = trControl)
?randomForest
rf_default
modelFit_2
confusionMatrix(rf_default, training$classe)
print(modelFit_2)
confusionMatrix(modelFit_2, training$classe)
confusionMatrix(modelFit_2, training$classe)
str(training)
str(modelFit_2)
confusionMatrix(modelFit_2, training)
confusionMatrix(modelFit_2, training$classe)
ok <- confusionMatrix(modelFit_2, training$classe)
modelFit_2$y
class(modelFit_2$y)
ok <- confusionMatrix(modelFit_2$y, training$classe)
ok
?rpart
control <- rpart.control(xval = 10)
rcontrol <- rpart.control(xval = 10)
modelFit_1 <- rpart(classe ~ ., data = training, method = "class", control = rcontrol)
modelFit_1 # Look at model
fancyRpartPlot(modelFit_1)
predictTreeMod1 <- predict(modelFit_1, testing, type = "class")
confusionMatrix(predictTreeMod1, testing$classe)
